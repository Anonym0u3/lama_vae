data:
  target: dataset.data_module.lamavaeDataModule
  params:
    # Path to training set configuration file.
    train_config: "/hy-tmp/lamavae/configs/dataset/lamavae_train.yaml"
    # Path to validation set configuration file.
    val_config:  "/hy-tmp/lamavae/configs/dataset/lamavae_val.yaml"

model:
  # You can set learning rate in the following configuration file.
  config: /hy-tmp/lamavae/configs/models/lamaVAE.yaml
  # Path to the checkpoints or weights you want to resume.
  resume: /hy-tmp/lamavae/checkpoints/steps=0.ckpt

lightning:
  seed: 666
  
  trainer:
    accelerator: gpu
    strategy: ddp
    precision: 32
    fast_dev_run: False # If true, only runs a few batches for debugging.
    # Indices of GPUs used for training.
    devices: [0, 1, 2, 3]
    #gpus: 1
    log_every_n_steps: 10
    # Max number of training steps (batches).
    max_steps: 2000000
    # Validation frequency in terms of training steps.
    val_check_interval: 250
    # Accumulate gradients from multiple batches so as to increase batch size.
    accumulate_grad_batches: 1

  callbacks:
    - target: model.callbacks.ModelCheckpoint
      params:
        # Frequency of saving checkpoints.
        every_n_train_steps: 2000
        save_weights_only: true
        save_top_k: -1
        filename: "{step}"
        dirpath: '/hy-tmp/lamavae/checkpoints'
    - target: model.callbacks.LogPredictionSamplesCallback
    - target: model.callbacks.MyPrintingCallback

